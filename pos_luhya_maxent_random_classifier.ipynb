{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdnahM/NLP_POS_Luhya/blob/main/pos_luhya_maxent_random_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PBwqNrRlCqy"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "### Natural Language Processing\n",
        "### Edna Wairimu Mugoh\n",
        "### C241-01-2293/2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoffocJqlHIX"
      },
      "source": [
        "Below project is a POS task on Luhya Dataset-Specialized with the Busukusu Speaking\n",
        "\n",
        "Project Parts\n",
        "1. Data preprocessing\n",
        "2. Training and Modelling\n",
        "3. Model Evaluation\n",
        "4. Developing a usable endpoint\n",
        "5. Deployed POS app for Bukusu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km5D8ybBlHZo"
      },
      "source": [
        "# Data Preporcessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "000Xu5XllxCE"
      },
      "source": [
        "### Import required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1-dqBC7A1Ft"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY6RxKQ-M4gS",
        "outputId": "bbff3182-60c8-4936-c9e4-9dc54d12a323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "scikit-learn version: 1.4.2\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade scikit-learn\n",
        "import sklearn\n",
        "# Check scikit-learn version\n",
        "print(\"scikit-learn version:\", sklearn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdscfQUCtgBy"
      },
      "source": [
        "### Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BjHF3qMuAWt",
        "outputId": "1db521b2-9e2c-4764-fad5-eae8b85c5343"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfaMuUotuAJL",
        "outputId": "c785fc6b-124e-4647-8a76-43b4dd03bfec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JFRu50xtjur"
      },
      "outputs": [],
      "source": [
        "train_df =  pd.read_csv(\"/content/drive/MyDrive/MSC-DS-2023/combined_bukusu_data.csv\", header=0)\n",
        "test_df =  pd.read_csv(\"/content/drive/MyDrive/MSC-DS-2023/combined_bukusu_test_data.csv\", header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zi1doD_lva63",
        "outputId": "c7b7d303-4c3e-4b82-8aa8-2e828e7a222b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        WORD SPEECH TAG\n",
              "0  chererere          X\n",
              "1    khubuna          V\n",
              "2       naba          V\n",
              "3  siloleela          V\n",
              "4  charebwao          V"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86d6cc3c-470e-4ac8-ad0e-404c958073e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WORD</th>\n",
              "      <th>SPEECH TAG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>chererere</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>khubuna</td>\n",
              "      <td>V</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>naba</td>\n",
              "      <td>V</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>siloleela</td>\n",
              "      <td>V</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>charebwao</td>\n",
              "      <td>V</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86d6cc3c-470e-4ac8-ad0e-404c958073e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86d6cc3c-470e-4ac8-ad0e-404c958073e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86d6cc3c-470e-4ac8-ad0e-404c958073e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4de4d9ce-2acc-441a-a092-78d42b566345\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4de4d9ce-2acc-441a-a092-78d42b566345')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4de4d9ce-2acc-441a-a092-78d42b566345 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14CZ0rIUWB6P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcEKrxUx-_-w",
        "outputId": "34704635-dc44-4251-9bf2-236b98f98643"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WORD          object\n",
              "SPEECH TAG    object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLwC0kx4_JmM",
        "outputId": "0e7e91a1-b864-4ee1-e1b9-b1cc311046e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(327747, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN37K6YkvfHr"
      },
      "source": [
        "### Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yADhOHiXva2L",
        "outputId": "8c8ab43f-edb2-40a0-96f0-3fdd4f177506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPEECH TAG\n",
            "NN            62585\n",
            "V             45300\n",
            "ADP           34083\n",
            "nn            19518\n",
            "CONJ          18650\n",
            "v             14988\n",
            "PRON          14471\n",
            "DET           14377\n",
            "PUNCT         12782\n",
            "conj          11364\n",
            "ADJ            9928\n",
            "ADV            6427\n",
            "punct          6426\n",
            "PUNC           5988\n",
            "adp            4944\n",
            "pron           4602\n",
            "NUM            3852\n",
            "adv            2670\n",
            "adj            2520\n",
            "PREP           1572\n",
            "det            1116\n",
            "num             858\n",
            "X               478\n",
            "POS             138\n",
            "PRO             108\n",
            "N                96\n",
            "ADJE             84\n",
            "inter            72\n",
            "x                66\n",
            "n                60\n",
            "AP               60\n",
            "pro              54\n",
            "con              48\n",
            "D                42\n",
            "Pron             42\n",
            "pr               36\n",
            "DP               24\n",
            "cv               24\n",
            "PART             24\n",
            "AD               24\n",
            "AV               24\n",
            "ART              18\n",
            "VV               18\n",
            "dp               18\n",
            "XX               18\n",
            "A                18\n",
            "AD[              12\n",
            "mm               12\n",
            "NU               12\n",
            "vv               12\n",
            "CONJU            12\n",
            "PUNT             12\n",
            "PR               12\n",
            "ñn               12\n",
            "prom             12\n",
            "nnn              12\n",
            "C0NJ             12\n",
            "ADDP             12\n",
            "INTER            12\n",
            "C                12\n",
            "ne               12\n",
            "NNN              12\n",
            "ADO              12\n",
            "nn4              12\n",
            "SPEECH TAG       12\n",
            "APD               6\n",
            "DJ                6\n",
            "HH                6\n",
            "NM                6\n",
            "pos               6\n",
            "PROUN             6\n",
            "asp               6\n",
            "av                6\n",
            "nu                6\n",
            "p                 6\n",
            "bakaambisi        6\n",
            "um                6\n",
            "asinyikhwa        6\n",
            "po                6\n",
            "b                 6\n",
            "Conj              6\n",
            "PR0N              6\n",
            "NOUN              6\n",
            "CHIRUPIA          6\n",
            "KHUKHWAMA         6\n",
            "NGA               6\n",
            "BULI              6\n",
            "MBOOLELE          6\n",
            "NO                6\n",
            "ON                6\n",
            "mala              6\n",
            "chambukha         6\n",
            "O                 6\n",
            "DV                6\n",
            "ABAAELESIA        6\n",
            "MASA              6\n",
            "HATATI            6\n",
            "TEMA              6\n",
            "YEMA              6\n",
            "YETURI            6\n",
            "DADV              6\n",
            "AADV              6\n",
            "OMUKHAANA         6\n",
            "NE                6\n",
            "PU                6\n",
            "AJ                6\n",
            "adk               6\n",
            "pun               6\n",
            "adadp             6\n",
            "ADp               6\n",
            "COJ               6\n",
            "ADDJ              6\n",
            "PRE               6\n",
            "OU                6\n",
            "AADJ              6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "pos_counts = train_df['SPEECH TAG'].value_counts()\n",
        "print(pos_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hfWs_c10MTm"
      },
      "source": [
        "#### Function to standardize the various variations of pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22lJzNo_zAPm"
      },
      "outputs": [],
      "source": [
        "def standardize_pos(pos):\n",
        "    \"\"\"\n",
        "    Standardize POS tags to basic presentation\n",
        "    \"\"\"\n",
        "    pos_mapping = {\n",
        "        'N': ['N', 'NN', 'NNN', 'NNNN', 'NOUN','noun', 'nn','NNC', 'NNS','nn4','NO','NU','ñn'], # Noun\n",
        "        'V': ['V', 'VB', 'verb', 'vb','v','VV', 'vv'], # Verb\n",
        "        'PRON':['PR','NNP','PROM','PRO','PRON', 'P','pro', 'PROUN','PR0N'], # Pronoun\n",
        "        'PUNCT':['punct','punc','PUNT'], # Punctuation\n",
        "        'ADJ': ['AD', 'ADJ','adj', 'AD [','ADO',' adje','AADJ','DJ','ADJE','ADDJ','AJ','AD['], # Adjective\n",
        "        'ADV': ['ADV','AV','adv','DV','AV', 'AADV','DADV'], # Adverb\n",
        "        'PREP':['PREP', 'prep','pre'], # Preposition\n",
        "        'CONJ': ['conju', 'conj','CON','C','c', 'COJ', 'C0NJ'], # Conjuction\n",
        "        'NUM': ['NUM', 'num',], # Integer\n",
        "        'DT': ['DET','DT','D'], # Determiner\n",
        "        'INTJ': ['inter',''], # Interjection\n",
        "        'XX' : ['XX', 'X'], # Unknown\n",
        "        'ADP' :['AP', 'ADP','adp','ADDP','addp','APD', 'adadp','dp'], # Adposition\n",
        "\n",
        "    }\n",
        "\n",
        "   # Checking Null POC\n",
        "    if pd.isna(pos):\n",
        "      pos = 'PUNCT'\n",
        "      return pos\n",
        "\n",
        "    pos_upper = pos.upper().strip()\n",
        "\n",
        "    for standard_pos, variations in pos_mapping.items():\n",
        "        for variation in variations:\n",
        "            if pos_upper == variation.upper():\n",
        "                return standard_pos\n",
        "\n",
        "    return pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA-MWljn0L_b"
      },
      "outputs": [],
      "source": [
        "def standardize_pos_column(df, column_name):\n",
        "    df[column_name] = df[column_name].apply(standardize_pos)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7NRYhOzACK"
      },
      "outputs": [],
      "source": [
        "cleaned_df = standardize_pos_column(train_df, 'SPEECH TAG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDw6IG5Ay_9P",
        "outputId": "34e33266-6c58-40e3-f0ff-1fee2a7033c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned  POS Dataframe\n",
            "SPEECH TAG\n",
            "N             82337\n",
            "V             60318\n",
            "PUNCT         51812\n",
            "ADP           39159\n",
            "CONJ          30110\n",
            "PRON          19355\n",
            "DT            15535\n",
            "ADJ           12604\n",
            "ADV            9145\n",
            "NUM            4710\n",
            "PREP           1578\n",
            "XX              562\n",
            "POS             138\n",
            "INTJ             84\n",
            "cv               24\n",
            "PART             24\n",
            "ART              18\n",
            "A                18\n",
            "ne               12\n",
            "mm               12\n",
            "SPEECH TAG       12\n",
            "adk               6\n",
            "pun               6\n",
            "chambukha         6\n",
            "mala              6\n",
            "um                6\n",
            "bakaambisi        6\n",
            "b                 6\n",
            "po                6\n",
            "asinyikhwa        6\n",
            "asp               6\n",
            "CHIRUPIA          6\n",
            "KHUKHWAMA         6\n",
            "NGA               6\n",
            "BULI              6\n",
            "MBOOLELE          6\n",
            "ON                6\n",
            "O                 6\n",
            "ABAAELESIA        6\n",
            "MASA              6\n",
            "HATATI            6\n",
            "TEMA              6\n",
            "YEMA              6\n",
            "YETURI            6\n",
            "OMUKHAANA         6\n",
            "NE                6\n",
            "PU                6\n",
            "OU                6\n",
            "HH                6\n",
            "NM                6\n",
            "pos               6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Display the filtered DataFrame\n",
        "pos_counts = cleaned_df['SPEECH TAG'].value_counts()\n",
        "print(\"Cleaned  POS Dataframe\")\n",
        "print(pos_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQzRmJnQy_uW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo2TcWZgxubK"
      },
      "source": [
        "#### Eliminate all rows with pos_count count of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDMxrt7AvazU"
      },
      "outputs": [],
      "source": [
        "train_df_v1 = train_df.copy()\n",
        "pos_to_keep = pos_counts[pos_counts != 6].index\n",
        "\n",
        "cleaned_df = train_df[train_df['SPEECH TAG'].isin(pos_to_keep)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqkRGdHHxt_i"
      },
      "outputs": [],
      "source": [
        "def filter_dataframe_by_words(df, column_name, words):\n",
        "    \"\"\"\n",
        "      Filter Dataframe\n",
        "    \"\"\"\n",
        "    mask = df[column_name].str.contains('|'.join(words), case=False)\n",
        "    return df[~mask]\n",
        "\n",
        "words_to_exclude = ['SPEECH TAG', 'POS', 'PART','cv','A', 'ART','mm','ne']\n",
        "\n",
        "cleaned_df = filter_dataframe_by_words(cleaned_df, 'SPEECH TAG', words_to_exclude)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJXILTz6hWRB",
        "outputId": "532b0abd-57bc-46df-8067-6f0142d7cb45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SPEECH TAG\n",
              "N        82337\n",
              "V        60318\n",
              "PUNCT    51812\n",
              "CONJ     30110\n",
              "PRON     19355\n",
              "DT       15535\n",
              "NUM       4710\n",
              "PREP      1578\n",
              "XX         562\n",
              "INTJ        84\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "cleaned_df['SPEECH TAG'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdwJsUyclYU6"
      },
      "source": [
        "#### Small visualization on the POS count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKAu34Mml3-M",
        "outputId": "f6bced16-9a82-44af-e877-197436e9bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned  POS Dataframe\n",
            "SPEECH TAG\n",
            "N        82337\n",
            "V        60318\n",
            "PUNCT    51812\n",
            "CONJ     30110\n",
            "PRON     19355\n",
            "DT       15535\n",
            "NUM       4710\n",
            "PREP      1578\n",
            "XX         562\n",
            "INTJ        84\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "pos_counts = cleaned_df['SPEECH TAG'].value_counts()\n",
        "print(\"Cleaned  POS Dataframe\")\n",
        "print(pos_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoz7lBs6lWhd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "colors = ['skyblue' if count >= 30000 else 'salmon' for count in pos_counts]\n",
        "pos_counts.plot(kind='bar', color=colors)\n",
        "\n",
        "plt.title('Frequency of Each Part of Speech', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Part of Speech', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.xticks(fontsize=12, rotation=45, ha='right')\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0xlyVgwKBus"
      },
      "outputs": [],
      "source": [
        "cleaned_df.to_csv(\"/content/drive/MyDrive/MSC-DS-2023/cleaned_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZa1MBK3Byd9"
      },
      "source": [
        "### Tokenization and Lemmatization on the given Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdcaFrBSCIAw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5U-Q6TWCn4g",
        "outputId": "dc8f541d-7474-4409-83e8-387ec4b9b269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXuuhoSZCNtg"
      },
      "outputs": [],
      "source": [
        "cleaned_df['WORD'] = cleaned_df['WORD'].astype(str)\n",
        "cleaned_df['TOKENS'] = cleaned_df['WORD'].apply(word_tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EO0kpNgCtuF"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm') ## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apLtEDMmCDar"
      },
      "outputs": [],
      "source": [
        "def lemmatize_text(text):\n",
        "    doc = nlp(\" \".join(text))\n",
        "    return [token.lemma_ for token in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikDPrZR4CDRz"
      },
      "outputs": [],
      "source": [
        "cleaned_df['LEMMAS'] = cleaned_df['TOKENS'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16H-JeiSCDFa",
        "outputId": "e2012d55-400c-4cab-c3b7-63fd0067a8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            WORD SPEECH TAG      TOKENS      LEMMAS\n",
            "327734    Bioosi         DT    [Bioosi]    [Bioosi]\n",
            "327735     beela      PUNCT     [beela]     [beela]\n",
            "327736   mukhola          V   [mukhola]   [mukhola]\n",
            "327737   bubwoni          N   [bubwoni]   [bubwoni]\n",
            "327739      Wele          N      [Wele]      [Wele]\n",
            "327741  Isiraeli          N  [Isiraeli]  [isiraeli]\n",
            "327742  babaandu          N  [babaandu]  [babaandu]\n",
            "327743      bowo       PRON      [bowo]      [bowo]\n",
            "327744      nibo       PRON      [nibo]      [nibo]\n",
            "327745  wareesia          V  [wareesia]  [wareesia]\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_df.tail(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZyvdtXJmTJ9"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sZZsHZY_oep"
      },
      "source": [
        "### Using the NLTK Library for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzkP44puA5y_"
      },
      "source": [
        "#### Install and Import the required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iMLMLCNZAzj3",
        "outputId": "22fcafdf-a197-4c44-c13a-524dd1cadc7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (3.20.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from stanfordnlp) (2.2.1+cu121)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.9.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->stanfordnlp) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->stanfordnlp)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.7)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading backports.tarfile-1.1.1-py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=937230bf1c3a531c646a99d64015c7e9461f23e47aedbea28f15c94414439ad2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124729 sha256=fdf456ed8593a80e1a962004cdd1974bb6f0d3293517a764ce0418e0ee718cf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=8274563185f56704584ca89dd0f43d4e09932c27bd113acf3d5bd0e2c3d28330\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaraco.context, cheroot, portend, pdfminer.six, nvidia-cusolver-cu12, jaraco.text, stanfordnlp, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.1.1 cheroot-10.0.1 cherrypy-18.9.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.0 mysqlclient-2.2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 stanfordnlp-0.2.0 tempora-5.5.1 zc.lockfile-3.0.post1\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk spacy textblob stanfordnlp pattern gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcHdHccFm7hb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "import stanfordnlp\n",
        "import pattern\n",
        "import gensim\n",
        "from nltk.corpus import treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIyQBtH6O4_6"
      },
      "source": [
        "#### Function to define POS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVdMWq7ZQkNf"
      },
      "outputs": [],
      "source": [
        "def features(word):\n",
        "    return {'word': word}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeszPQG7Qpyu"
      },
      "outputs": [],
      "source": [
        "# Extract features from your dataset\n",
        "data = []\n",
        "for index, row in cleaned_df.iterrows():\n",
        "    word = row['WORD']\n",
        "    tag = row['SPEECH TAG']\n",
        "    featureset = features(word)\n",
        "    data.append((featureset, tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMzXs_RKQzpa"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "train_size = int(0.8 * len(data))\n",
        "train_set, test_set = data[:train_size], data[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-NmyYoG5TwW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObGuqXD-5Uj5"
      },
      "source": [
        "#### Maxentropy Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oOdtUecQzdR",
        "outputId": "481cd557-8956-4793-c254-90ebdff89114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (100 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -2.30259        0.002\n",
            "             2          -0.67039        0.937\n",
            "             3          -0.45442        0.937\n",
            "             4          -0.35673        0.937\n",
            "             5          -0.30069        0.937\n",
            "             6          -0.26427        0.937\n",
            "             7          -0.23867        0.937\n",
            "             8          -0.21969        0.937\n",
            "             9          -0.20505        0.937\n",
            "            10          -0.19341        0.937\n",
            "            11          -0.18394        0.937\n",
            "            12          -0.17607        0.937\n",
            "            13          -0.16944        0.937\n",
            "            14          -0.16377        0.937\n",
            "            15          -0.15887        0.937\n",
            "            16          -0.15459        0.937\n",
            "            17          -0.15082        0.937\n",
            "            18          -0.14747        0.937\n",
            "            19          -0.14448        0.937\n",
            "            20          -0.14179        0.937\n",
            "            21          -0.13937        0.937\n",
            "            22          -0.13716        0.937\n",
            "            23          -0.13515        0.937\n",
            "            24          -0.13331        0.937\n",
            "            25          -0.13161        0.937\n",
            "            26          -0.13005        0.937\n",
            "            27          -0.12860        0.937\n",
            "            28          -0.12726        0.937\n",
            "            29          -0.12601        0.937\n",
            "            30          -0.12485        0.937\n",
            "            31          -0.12376        0.937\n",
            "            32          -0.12274        0.937\n",
            "            33          -0.12178        0.937\n",
            "            34          -0.12088        0.937\n",
            "            35          -0.12003        0.937\n",
            "            36          -0.11922        0.937\n",
            "            37          -0.11846        0.937\n",
            "            38          -0.11774        0.937\n",
            "            39          -0.11706        0.937\n",
            "            40          -0.11642        0.937\n",
            "            41          -0.11580        0.937\n",
            "            42          -0.11521        0.937\n",
            "            43          -0.11466        0.937\n",
            "            44          -0.11412        0.937\n",
            "            45          -0.11361        0.937\n",
            "            46          -0.11312        0.937\n",
            "            47          -0.11266        0.937\n",
            "            48          -0.11221        0.937\n",
            "            49          -0.11178        0.937\n",
            "            50          -0.11137        0.937\n",
            "            51          -0.11098        0.937\n",
            "            52          -0.11060        0.937\n"
          ]
        }
      ],
      "source": [
        "classifier = nltk.MaxentClassifier.train(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P82lQeE-QzQ3"
      },
      "outputs": [],
      "source": [
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMwMasK8p1A_"
      },
      "source": [
        "#### Save the predictions to a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2vxxKSnm8LV"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('maxentpos_tagger.pickle', 'wb') as f:\n",
        "    pickle.dump(classifier, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31oDcXCc5Qtz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvic_0uc5cmw"
      },
      "source": [
        "#### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdjOONUUoK6P"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction import DictVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb6KzhV45Qb9"
      },
      "outputs": [],
      "source": [
        "def features(sentence, index):\n",
        "    word = sentence[index]\n",
        "    featureset = {'word': word,\n",
        "                  'word_length': len(word),\n",
        "                  'has_numbers': any(char.isdigit() for char in word),\n",
        "                  'has_special_chars': any(not char.isalnum() for char in word),\n",
        "                  'position_in_sentence': index}\n",
        "    return featureset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjqNWVI6mFB8"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for index, row in cleaned_df.iterrows():\n",
        "    word = row['WORD']\n",
        "    tag = row['SPEECH TAG']\n",
        "    featureset = features(word_tokenize(word), 0)\n",
        "    data.append((featureset, tag))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzsR_B4amNDq"
      },
      "outputs": [],
      "source": [
        "X = [d[0] for d in data]\n",
        "y = [d[1] for d in data]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPWQiKTumysC"
      },
      "source": [
        "#### Using the Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHMezGE0uLuD"
      },
      "outputs": [],
      "source": [
        "vectorizer = DictVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfrggdtymNA3"
      },
      "outputs": [],
      "source": [
        "\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train_vec, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1Dp56hLuwIw"
      },
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_ifLSjyuyeX"
      },
      "outputs": [],
      "source": [
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "y_pred = classifier.predict(X_test_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MT4AhMbb3DD"
      },
      "outputs": [],
      "source": [
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwZWWWDGmpgJ"
      },
      "source": [
        "#### Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzGYlybmmM5d"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWxBrm0mYsp"
      },
      "source": [
        "#### Hyperpaparmeter Tuning\n",
        "Below using the 5-folds to improve on the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEIxw0UKmMt1"
      },
      "outputs": [],
      "source": [
        "X_vec = vectorizer.fit_transform(X)\n",
        "cv_scores = cross_val_score(classifier, X_vec, y, cv=10)\n",
        "print(\"Cross-Validation Mean Accuracy:\", cv_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLGzuA7ab9-R"
      },
      "outputs": [],
      "source": [
        "print(X_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnT1ECuLb91z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqr-3c-GnC4o"
      },
      "source": [
        "#### Saving the models Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ0lIEs1nGb_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/MSC-DS-2023/rf_pos_tagger.pickle', 'wb') as f:\n",
        "    pickle.dump(classifier, f,protocol=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLl2uru3nGe5"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import sklearn\n",
        "\n",
        "# Check scikit-learn version\n",
        "print(\"scikit-learn version:\", sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FMc_tq1nGZm"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/MSC-DS-2023/rfvect_pos_tagger.pickle', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f,protocol=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZqPyIne_vWF"
      },
      "source": [
        "### Using Neural Networks for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNnOu_99mEfe"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoK1gqxEm80T"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIzLNeLxnWVh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NS49RAgnW3P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-Ftfp1nWrZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Wl5M5hnXak"
      },
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qoK1gqxEm80T"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMQGPHNAfKFfsocvycSZNEI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}